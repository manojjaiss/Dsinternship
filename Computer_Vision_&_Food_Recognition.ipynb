{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEKgPsmvu5HYn0Pu15Z+2w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manojjaiss/Dsinternship/blob/main/Computer_Vision_%26_Food_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up Your Google Colab Environment**"
      ],
      "metadata": {
        "id": "MGiwtqqIXBUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB_inCu2gsFj",
        "outputId": "d476bea8-611e-4597-f4a0-a03b4b02ce06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow matplotlib scikit-learn pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download and Extract the Food-101 Dataset**"
      ],
      "metadata": {
        "id": "zTO3huo6XEye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "# Create a directory to store data\n",
        "data_dir = \"food-101\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# Download the dataset\n",
        "url = \"http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\"\n",
        "filename = os.path.join(data_dir, \"food-101.tar.gz\")\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Extract dataset\n",
        "with tarfile.open(filename, \"r:gz\") as tar:\n",
        "    tar.extractall(path=data_dir)\n",
        "    print(\"Extraction complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WuUn5WrlW71",
        "outputId": "5d227940-64d2-4650-959c-4f1e8a3f3b29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare the Dataset for CNN**\n",
        ""
      ],
      "metadata": {
        "id": "n6MOKwp5XH1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data paths and split"
      ],
      "metadata": {
        "id": "oA1cmfpXXSjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "dataset_path = \"food-101/food-101/images\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDDgfCFhnFDP",
        "outputId": "1e2354e8-998b-4285-a7e2-66ca9af76063"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 80800 images belonging to 101 classes.\n",
            "Found 20200 images belonging to 101 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Build and Train CNN Model**"
      ],
      "metadata": {
        "id": "AaMuvGbfXXSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " CNN using TensorFlow"
      ],
      "metadata": {
        "id": "Tfn0OexDXbnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "R7SQO8E0nMOd",
        "outputId": "d5e7f74e-679d-4519-94b5-f71fc6251553"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57600\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m14,745,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m)            │        \u001b[38;5;34m25,957\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57600</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,745,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,957</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,791,205\u001b[0m (56.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,791,205</span> (56.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,791,205\u001b[0m (56.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,791,205</span> (56.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model**\n",
        "\n"
      ],
      "metadata": {
        "id": "LOIPMGKLXhDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0grnMPZnPNQ",
        "outputId": "ab965e2c-78e6-4d13-df44-304b7eebe237"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2908s\u001b[0m 1s/step - accuracy: 0.0231 - loss: 4.5699 - val_accuracy: 0.0837 - val_loss: 4.1357\n",
            "Epoch 2/10\n",
            "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2952s\u001b[0m 1s/step - accuracy: 0.0818 - loss: 4.1191 - val_accuracy: 0.1272 - val_loss: 3.8469\n",
            "Epoch 3/10\n",
            "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2897s\u001b[0m 1s/step - accuracy: 0.1258 - loss: 3.8043 - val_accuracy: 0.1388 - val_loss: 3.7787\n",
            "Epoch 4/10\n",
            "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2866s\u001b[0m 1s/step - accuracy: 0.1716 - loss: 3.5188 - val_accuracy: 0.1573 - val_loss: 3.6726\n",
            "Epoch 5/10\n",
            "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2889s\u001b[0m 1s/step - accuracy: 0.2265 - loss: 3.1934 - val_accuracy: 0.1556 - val_loss: 3.7051\n",
            "Epoch 6/10\n",
            "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2903s\u001b[0m 1s/step - accuracy: 0.2839 - loss: 2.8644 - val_accuracy: 0.1472 - val_loss: 3.8068\n",
            "Epoch 7/10\n",
            "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2875s\u001b[0m 1s/step - accuracy: 0.3472 - loss: 2.5397 - val_accuracy: 0.1446 - val_loss: 3.9461\n",
            "Epoch 8/10\n",
            "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2904s\u001b[0m 1s/step - accuracy: 0.4079 - loss: 2.2306 - val_accuracy: 0.1359 - val_loss: 4.2450\n",
            "Epoch 9/10\n",
            "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2884s\u001b[0m 1s/step - accuracy: 0.4698 - loss: 1.9469 - val_accuracy: 0.1315 - val_loss: 4.4979\n",
            "Epoch 10/10\n",
            "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2897s\u001b[0m 1s/step - accuracy: 0.5172 - loss: 1.7366 - val_accuracy: 0.1291 - val_loss: 4.7630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict Food Class**"
      ],
      "metadata": {
        "id": "vBdpYuv5XqQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "def predict_food(img_path):\n",
        "    img = image.load_img(img_path, target_size=(128, 128))\n",
        "    x = image.img_to_array(img) / 255.0\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    pred = model.predict(x)\n",
        "    class_index = np.argmax(pred)\n",
        "    class_label = list(train_generator.class_indices.keys())[class_index]\n",
        "    return class_label\n",
        "\n",
        "# Example\n",
        "predict_food('food-101/food-101/images/sushi/1238583.jpg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wYPRNqcJKgOx",
        "outputId": "fcc0dbd6-9af8-49fd-a273-dad866c4d248"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'deviled_eggs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict Salt Content Based on Food Class**"
      ],
      "metadata": {
        "id": "mmg4D9MEebrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Salt Content Dataset (food_raw.csv)"
      ],
      "metadata": {
        "id": "N_TAet5eeuBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace with your actual path if needed\n",
        "df = pd.read_csv('/content/food_raw.csv')\n",
        "\n",
        "# View the column names\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n9JbY2DaNkq",
        "outputId": "12e38312-26e4-4605-f10f-fbd0080e4bcf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['No.', 'Food_ID', 'Nama Bahan Makanan', 'Fast_Food', 'Sumber', 'Tipe',\n",
            "       'Jenis_Olahan', 'Air (g)', 'Energi (Kal)', 'Protein (g)', 'Lemak (g)',\n",
            "       'Karbohidrat (g)', 'Serat (g)', 'Abu (g)', 'Kalsium (Ca) (mg)',\n",
            "       'Fosfor (P) (mg)', 'Besi (Fe) (mg)', 'Natrium (Na) (mg)',\n",
            "       'Kalium (Ka) (mg)', 'Tembaga (Cu) (mg)', 'Seng (Zn) (mg)',\n",
            "       'Retinol (vit. A) (mcg)', 'β-karoten (mcg)', 'Karoten total (mcg)',\n",
            "       'Thiamin (vit. B1) (mg)', 'Riboflavin (vit. B2) (mg)', 'Niasin (mg)',\n",
            "       'Vitamin C (mg)', 'BDD (%)', 'Mentah / Olahan', 'Kelompok Makanan',\n",
            "       'Sumber TKPI 2019', 'Fast_Food_New', 'Tipe_New', 'Jenis_Olahan_New',\n",
            "       'Mentah / Olahan_New', 'Kelompok Makanan_New', 'Fast_Food_New_0',\n",
            "       'Fast_Food_New_1', 'Tipe_New_0', 'Tipe_New_1', 'Jenis_Olahan_New_0',\n",
            "       'Jenis_Olahan_New_1', 'Jenis_Olahan_New_2', 'Jenis_Olahan_New_3',\n",
            "       'Jenis_Olahan_New_4', 'Jenis_Olahan_New_5', 'Jenis_Olahan_New_6',\n",
            "       'Jenis_Olahan_New_7', 'Jenis_Olahan_New_8', 'Jenis_Olahan_New_9',\n",
            "       'Jenis_Olahan_New_10', 'Mentah / Olahan_New_0', 'Mentah / Olahan_New_1',\n",
            "       'Kelompok Makanan_New_0', 'Kelompok Makanan_New_1',\n",
            "       'Kelompok Makanan_New_2', 'Kelompok Makanan_New_3',\n",
            "       'Kelompok Makanan_New_4', 'Kelompok Makanan_New_5',\n",
            "       'Kelompok Makanan_New_6', 'Kelompok Makanan_New_7',\n",
            "       'Kelompok Makanan_New_8', ' Daging Ayam', ' Daging Kambing',\n",
            "       ' Daging Kerbau', ' Daging Sapi', ' Ikan', ' Kedelai', ' Sayur',\n",
            "       ' Telur', 'Beras', 'Biji-bijian', 'Buah', 'Daging Ayam', 'Daging Babi',\n",
            "       'Daging Kambing', 'Daging Sapi', 'Ikan', 'Kedelai', 'Sayur', 'Susu',\n",
            "       'Telur Ayam', 'Tepung', 'Umbi-umbian'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Lookup Function for Salt Content"
      ],
      "metadata": {
        "id": "0mJN0Fgreyo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_salt_content(food_label):\n",
        "    # Match predicted food name with entries in the dataset\n",
        "    matches = df[df['food_name'].str.lower().str.contains(food_label.lower())]\n",
        "\n",
        "    if not matches.empty:\n",
        "        avg_salt = matches['sodium'].mean()\n",
        "        return round(avg_salt, 2)\n",
        "    else:\n",
        "        return \"Not available\"\n"
      ],
      "metadata": {
        "id": "-sOI-yjXaVlV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust Electrical Stimulation Based on Salt"
      ],
      "metadata": {
        "id": "zJIHiDI4e3Yr"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_salt_content(food_label):\n",
        "    \"\"\"\n",
        "    Gets the average salt content for a given food label from the dataframe.\n",
        "\n",
        "    Args:\n",
        "        food_label (str): The name of the food.\n",
        "\n",
        "    Returns:\n",
        "        float: The average salt content in grams, or \"Not available\" if not found.\n",
        "    \"\"\"\n",
        "    # Assuming 'Nama Bahan Makanan' is the correct column for food names\n",
        "    matches = df[df['Nama Bahan Makanan'].str.lower().str.contains(food_label.lower())]\n",
        "\n",
        "    if not matches.empty:\n",
        "        # Assuming 'Sodium (mg)' is the correct column for sodium content\n",
        "        avg_salt = matches['Sodium (mg)'].mean()\n",
        "        # Convert milligrams to grams\n",
        "        return round(avg_salt / 1000, 2)\n",
        "    else:\n",
        "        return \"Not available\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UZrRSutzauTk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Electrical stimulation Example"
      ],
      "metadata": {
        "id": "4-MYY_ufe7Kj"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_salt_content(food_label):\n",
        "    \"\"\"\n",
        "    Gets the average salt content for a given food label from the dataframe.\n",
        "\n",
        "    Args:\n",
        "        food_label (str): The name of the food.\n",
        "\n",
        "    Returns:\n",
        "        float: The average salt content in grams, or \"Not available\" if not found.\n",
        "    \"\"\"\n",
        "    # Assuming 'Nama Bahan Makanan' is the correct column for food names\n",
        "    matches = df[df['Nama Bahan Makanan'].str.lower().str.contains(food_label.lower())]\n",
        "\n",
        "    if not matches.empty:\n",
        "        # Assuming 'Sodium (mg)' is the correct column for sodium content\n",
        "        avg_salt = matches['Sodium (mg)'].mean()\n",
        "        # Convert milligrams to grams\n",
        "        return round(avg_salt / 1000, 2)\n",
        "    else:\n",
        "        return \"Not available\"\n",
        "\n",
        "def adjust_stimulation(salt_content):\n",
        "    if salt_content == \"Not available\":\n",
        "        return \"Default stimulation mode\"\n",
        "    elif salt_content > 2.0:\n",
        "        return \"Reduce stimulation intensity\"\n",
        "    elif salt_content < 0.5:\n",
        "        return \"Increase stimulation intensity\"\n",
        "    else:\n",
        "        return \"Normal stimulation\"\n",
        "\n",
        "# Predict the food from an example image\n",
        "predicted_food = predict_food('food-101/food-101/images/sushi/1238583.jpg')\n",
        "\n",
        "# Get the salt estimate for the predicted food\n",
        "salt_estimate = get_salt_content(predicted_food) # Assigning the result to salt_estimate\n",
        "\n",
        "# Example\n",
        "stim_msg = adjust_stimulation(salt_estimate)\n",
        "print(f\"Electrical stimulation: {stim_msg}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPBbJqeNa_-k",
        "outputId": "240572f4-c2d2-4536-ae97-af5cc45188b3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "Electrical stimulation: Default stimulation mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Combined Pipeline Example"
      ],
      "metadata": {
        "id": "NoDYTuQ-fUNZ"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your CSV file is named 'food_raw.csv' and located in the current directory\n",
        "df = pd.read_csv('/content/food_raw.csv')\n",
        "\n",
        "# View the column names to verify if 'Sodium (mg)' exists\n",
        "print(df.columns)\n",
        "\n",
        "# Get a random food item from the dataset\n",
        "food_item = df.sample(1)\n",
        "\n",
        "# Extract the food name and salt content using the correct column name\n",
        "food_name = food_item['Nama Bahan Makanan'].values[0]\n",
        "\n",
        "# Check if 'Sodium (mg)' exists in the columns before accessing it\n",
        "if 'Sodium (mg)' in food_item.columns:\n",
        "    salt_content_mg = food_item['Sodium (mg)'].values[0]\n",
        "else:\n",
        "    # Handle the case where the column is not found, e.g., print an error message or assign a default value\n",
        "    print(\"Error: 'Sodium (mg)' column not found in the dataframe.\")\n",
        "    salt_content_mg = 0  # Assign a default value or handle the error appropriately\n",
        "\n",
        "\n",
        "# Convert salt content to grams\n",
        "salt_content_g = salt_content_mg / 1000\n",
        "\n",
        "# Print the information\n",
        "print(f\"Food: {food_name}\")\n",
        "print(f\"Salt content (mg): {salt_content_mg}\")\n",
        "print(f\"Salt content (g): {salt_content_g:.2f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq18WUTIbksx",
        "outputId": "3583b636-015b-4f5a-ad6c-f62299b835e8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['No.', 'Food_ID', 'Nama Bahan Makanan', 'Fast_Food', 'Sumber', 'Tipe',\n",
            "       'Jenis_Olahan', 'Air (g)', 'Energi (Kal)', 'Protein (g)', 'Lemak (g)',\n",
            "       'Karbohidrat (g)', 'Serat (g)', 'Abu (g)', 'Kalsium (Ca) (mg)',\n",
            "       'Fosfor (P) (mg)', 'Besi (Fe) (mg)', 'Natrium (Na) (mg)',\n",
            "       'Kalium (Ka) (mg)', 'Tembaga (Cu) (mg)', 'Seng (Zn) (mg)',\n",
            "       'Retinol (vit. A) (mcg)', 'β-karoten (mcg)', 'Karoten total (mcg)',\n",
            "       'Thiamin (vit. B1) (mg)', 'Riboflavin (vit. B2) (mg)', 'Niasin (mg)',\n",
            "       'Vitamin C (mg)', 'BDD (%)', 'Mentah / Olahan', 'Kelompok Makanan',\n",
            "       'Sumber TKPI 2019', 'Fast_Food_New', 'Tipe_New', 'Jenis_Olahan_New',\n",
            "       'Mentah / Olahan_New', 'Kelompok Makanan_New', 'Fast_Food_New_0',\n",
            "       'Fast_Food_New_1', 'Tipe_New_0', 'Tipe_New_1', 'Jenis_Olahan_New_0',\n",
            "       'Jenis_Olahan_New_1', 'Jenis_Olahan_New_2', 'Jenis_Olahan_New_3',\n",
            "       'Jenis_Olahan_New_4', 'Jenis_Olahan_New_5', 'Jenis_Olahan_New_6',\n",
            "       'Jenis_Olahan_New_7', 'Jenis_Olahan_New_8', 'Jenis_Olahan_New_9',\n",
            "       'Jenis_Olahan_New_10', 'Mentah / Olahan_New_0', 'Mentah / Olahan_New_1',\n",
            "       'Kelompok Makanan_New_0', 'Kelompok Makanan_New_1',\n",
            "       'Kelompok Makanan_New_2', 'Kelompok Makanan_New_3',\n",
            "       'Kelompok Makanan_New_4', 'Kelompok Makanan_New_5',\n",
            "       'Kelompok Makanan_New_6', 'Kelompok Makanan_New_7',\n",
            "       'Kelompok Makanan_New_8', ' Daging Ayam', ' Daging Kambing',\n",
            "       ' Daging Kerbau', ' Daging Sapi', ' Ikan', ' Kedelai', ' Sayur',\n",
            "       ' Telur', 'Beras', 'Biji-bijian', 'Buah', 'Daging Ayam', 'Daging Babi',\n",
            "       'Daging Kambing', 'Daging Sapi', 'Ikan', 'Kedelai', 'Sayur', 'Susu',\n",
            "       'Telur Ayam', 'Tepung', 'Umbi-umbian'],\n",
            "      dtype='object')\n",
            "Error: 'Sodium (mg)' column not found in the dataframe.\n",
            "Food: Lapis legit\n",
            "Salt content (mg): 0\n",
            "Salt content (g): 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Automated Dietary Recommendation System***"
      ],
      "metadata": {
        "id": "VfdfTpLub5Xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Dietary Rules & Recommendations Logic*"
      ],
      "metadata": {
        "id": "NXDZ6gNPcAhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dietary_recommendation(food_label):\n",
        "    matches = df[df['food_name'].str.lower().str.contains(food_label.lower())]\n",
        "\n",
        "    if matches.empty:\n",
        "        return \"⚠️ No data found. Please consume in moderation.\"\n",
        "\n",
        "    # Take averages in case multiple rows match\n",
        "    avg_salt = matches['sodium'].mean()\n",
        "    avg_calories = matches['calories'].mean()\n",
        "    avg_fat = matches['fat'].mean()\n",
        "\n",
        "    advice = f\"🍽️ Nutrition for {food_label.title()}:\\n\"\n",
        "    advice += f\"   - Salt: {avg_salt:.2f}g\\n\"\n",
        "    advice += f\"   - Calories: {avg_calories:.0f} kcal\\n\"\n",
        "    advice += f\"   - Fat: {avg_fat:.2f}g\\n\\n\"\n",
        "\n",
        "    # Simple rules\n",
        "    if avg_salt > 2.0 or avg_fat > 20 or avg_calories > 500:\n",
        "        advice += \"⚠️ High in salt/fat/calories. Recommend limited intake.\\n\"\n",
        "    else:\n",
        "        advice += \"✅ Balanced choice. Safe for regular consumption.\\n\"\n",
        "\n",
        "    # Suggest healthy substitutes (example-based)\n",
        "    substitutions = {\n",
        "        \"burger\": \"grilled chicken sandwich\",\n",
        "        \"pizza\": \"vegetable flatbread\",\n",
        "        \"fried\": \"steamed or grilled dishes\",\n",
        "        \"samosa\": \"baked veggie roll\"\n",
        "    }\n",
        "\n",
        "    for word, sub in substitutions.items():\n",
        "        if word in food_label.lower():\n",
        "            advice += f\"💡 Try a healthier option like: *{sub.title()}*\\n\"\n",
        "\n",
        "    return advice\n"
      ],
      "metadata": {
        "id": "wQbiffmsbtmV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Full Integrated Pipeline***"
      ],
      "metadata": {
        "id": "nzVsQRujbyl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def full_diet_pipeline(image_path):\n",
        "    print(\"🔍 Identifying food...\")\n",
        "    label = predict_food(image_path)\n",
        "    print(f\"🍕 Predicted food: {label}\\n\")\n",
        "\n",
        "    salt = get_salt_content(label)\n",
        "    stim = adjust_stimulation(salt)\n",
        "    print(f\"🧂 Salt content: {salt}g\")\n",
        "    print(f\"⚡ Electrical Adjustment: {stim}\\n\")\n",
        "\n",
        "    # Nutrition + dietary advice\n",
        "    print(dietary_recommendation(label))\n"
      ],
      "metadata": {
        "id": "yDlWyPifbxPI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*pipeline_arjustment*"
      ],
      "metadata": {
        "id": "PjqeNCE5cGe5"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "def dietary_recommendation(food_label):\n",
        "    \"\"\"\n",
        "    Provides dietary recommendations based on the food label.\n",
        "\n",
        "    Args:\n",
        "        food_label (str): The name of the food.\n",
        "\n",
        "    Returns:\n",
        "        str: Dietary advice for the food.\n",
        "    \"\"\"\n",
        "    # Access the correct column name for food names, assuming it's 'Nama Bahan Makanan'\n",
        "    matches = df[df['Nama Bahan Makanan'].str.lower().str.contains(food_label.lower())]\n",
        "\n",
        "    if matches.empty:\n",
        "        return \"⚠️ No data found. Please consume in moderation.\"\n",
        "\n",
        "    # Assuming correct column names for sodium, calories, and fat\n",
        "    avg_salt = matches['Sodium (mg)'].mean() / 1000  # Convert to grams\n",
        "    avg_calories = matches['Energi (kal)'].mean()\n",
        "    avg_fat = matches['Lemak (g)'].mean()\n",
        "\n",
        "    advice = f\"🍽️ Nutrition for {food_label.title()}:\\n\"\n",
        "    advice += f\"   - Salt: {avg_salt:.2f}g\\n\"\n",
        "    advice += f\"   - Calories: {avg_calories:.0f} kcal\\n\"\n",
        "    advice += f\"   - Fat: {avg_fat:.2f}g\\n\\n\"\n",
        "\n",
        "    # Simple rules\n",
        "    if avg_salt > 2.0 or avg_fat > 20 or avg_calories > 500:\n",
        "        advice += \"⚠️ High in salt/fat/calories. Recommend limited intake.\\n\"\n",
        "    else:\n",
        "        advice += \"✅ Balanced choice. Safe for regular consumption.\\n\"\n",
        "\n",
        "    # Suggest healthy substitutes (example-based)\n",
        "    substitutions = {\n",
        "        \"burger\": \"grilled chicken sandwich\",\n",
        "        \"pizza\": \"vegetable flatbread\",\n",
        "        \"fried\": \"steamed or grilled dishes\",\n",
        "        \"samosa\": \"baked veggie roll\"\n",
        "    }\n",
        "\n",
        "    for word, sub in substitutions.items():\n",
        "        if word in food_label.lower():\n",
        "            advice += f\"💡 Try a healthier option like: *{sub.title()}*\\n\"\n",
        "\n",
        "    return advice"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "W6ROYeVGcssy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*full_diet_pipeline*"
      ],
      "metadata": {
        "id": "nymBcN_0eBSW"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def full_diet_pipeline(image_path, model, train_generator, df):\n",
        "    \"\"\"\n",
        "    Performs a full dietary analysis pipeline on an image of food.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "        model: The trained Keras model for food prediction.\n",
        "        train_generator: The Keras ImageDataGenerator used for training.\n",
        "        df: The Pandas DataFrame containing food nutritional information.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the predicted food, salt content, electrical stimulation adjustment,\n",
        "              and dietary recommendations.\n",
        "    \"\"\"\n",
        "    print(\"🔍 Identifying food...\")\n",
        "    label = predict_food(image_path, model, train_generator)  # Pass necessary arguments\n",
        "    print(f\"🍕 Predicted food: {label}\\n\")\n",
        "\n",
        "    salt = get_salt_content(label, df)  # Pass df as argument\n",
        "    stim = adjust_stimulation(salt)\n",
        "    print(f\"🧂 Salt content: {salt}g\")\n",
        "    print(f\"⚡ Electrical Adjustment: {stim}\\n\")\n",
        "\n",
        "    # Nutrition + dietary advice\n",
        "    print(dietary_recommendation(label, df))  # Pass df as argument\n",
        "\n",
        "\n",
        "def predict_food(img_path, model, train_generator):\n",
        "    \"\"\"\n",
        "    Predicts the food in an image using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        img_path (str): Path to the image.\n",
        "        model: The trained Keras model.\n",
        "        train_generator: The Keras ImageDataGenerator used for training.\n",
        "\n",
        "    Returns:\n",
        "        str: The predicted food label.\n",
        "    \"\"\"\n",
        "    img = image.load_img(img_path, target_size=(128, 128))\n",
        "    x = image.img_to_array(img) / 255.0\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    pred = model.predict(x)\n",
        "    class_index = np.argmax(pred)\n",
        "    class_label = list(train_generator.class_indices.keys())[class_index]\n",
        "    return class_label\n",
        "\n",
        "\n",
        "def get_salt_content(food_label, df):\n",
        "    \"\"\"\n",
        "    Gets the average salt content for a given food label from the dataframe.\n",
        "\n",
        "    Args:\n",
        "        food_label (str): The name of the food.\n",
        "        df: The Pandas DataFrame containing nutritional information.\n",
        "\n",
        "    Returns:\n",
        "        float: The average salt content in grams, or \"Not available\" if not found.\n",
        "    \"\"\"\n",
        "    matches = df[df['Nama Bahan Makanan'].str.lower().str.contains(food_label.lower())]\n",
        "\n",
        "    if not matches.empty:\n",
        "        avg_salt = matches['Sodium (mg)'].mean()\n",
        "        return round(avg_salt / 1000, 2)  # Convert to grams\n",
        "    else:\n",
        "        return \"Not available\"\n",
        "\n",
        "\n",
        "def adjust_stimulation(salt_content):\n",
        "    \"\"\"\n",
        "    Adjusts the electrical stimulation intensity based on salt content.\n",
        "\n",
        "    Args:\n",
        "        salt_content (float): The salt content of the food in grams.\n",
        "\n",
        "    Returns:\n",
        "        str: A message indicating the stimulation adjustment.\n",
        "    \"\"\"\n",
        "    if salt_content == \"Not available\":\n",
        "        return \"Default stimulation mode\"\n",
        "    elif salt_content > 2.0:\n",
        "        return \"Reduce stimulation intensity\"\n",
        "    elif salt_content < 0.5:\n",
        "        return \"Increase stimulation intensity\"\n",
        "    else:\n",
        "        return \"Normal stimulation\"\n",
        "\n",
        "\n",
        "def dietary_recommendation(food_label, df):\n",
        "    \"\"\"\n",
        "    Provides dietary recommendations based on the food label.\n",
        "\n",
        "    Args:\n",
        "        food_label (str): The name of the food.\n",
        "        df: The Pandas DataFrame containing nutritional information.\n",
        "\n",
        "    Returns:\n",
        "        str: Dietary advice for the food.\n",
        "    \"\"\"\n",
        "    matches = df[df['Nama Bahan Makanan'].str.lower().str.contains(food_label.lower())]\n",
        "\n",
        "    if matches.empty:\n",
        "        return \"⚠️ No data found. Please consume in moderation.\"\n",
        "\n",
        "    avg_salt = matches['Sodium (mg)'].mean() / 1000  # Convert to grams\n",
        "    avg_calories = matches['Energi (kal)'].mean()\n",
        "    avg_fat = matches['Lemak (g)'].mean()\n",
        "\n",
        "    advice = f\"🍽️ Nutrition for {food_label.title()}:\\n\"\n",
        "    advice += f\"   - Salt: {avg_salt:.2f}g\\n\"\n",
        "    advice += f\"   - Calories: {avg_calories:.0f} kcal\\n\"\n",
        "    advice += f\"   - Fat: {avg_fat:.2f}g\\n\\n\"\n",
        "\n",
        "    if avg_salt > 2.0 or avg_fat > 20 or avg_calories > 500:\n",
        "        advice += \"⚠️ High in salt/fat/calories. Recommend limited intake.\\n\"\n",
        "    else:\n",
        "        advice += \"✅ Balanced choice. Safe for regular consumption.\\n\"\n",
        "\n",
        "    substitutions = {\n",
        "        \"burger\": \"grilled chicken sandwich\",\n",
        "        \"pizza\": \"vegetable flatbread\",\n",
        "        \"fried\": \"steamed or grilled dishes\",\n",
        "        \"samosa\": \"baked veggie roll\"\n",
        "    }\n",
        "\n",
        "    for word, sub in substitutions.items():\n",
        "        if word in food_label.lower():\n",
        "            advice += f\"💡 Try a healthier option like: *{sub.title()}*\\n\"\n",
        "\n",
        "    return advice"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "W_o42-mmdbd0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*example*"
      ],
      "metadata": {
        "id": "jGPkMsXbd6MG"
      }
    },
    {
      "source": [
        "# Assuming you have loaded your model, train_generator, and df\n",
        "full_diet_pipeline('food-101/food-101/images/french_fries/1020588.jpg', model, train_generator, df)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdPmkoo9dgkC",
        "outputId": "35cf8cfa-4c59-436d-e92b-b537ee42e1d3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Identifying food...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "🍕 Predicted food: french_fries\n",
            "\n",
            "🧂 Salt content: Not availableg\n",
            "⚡ Electrical Adjustment: Default stimulation mode\n",
            "\n",
            "⚠️ No data found. Please consume in moderation.\n"
          ]
        }
      ]
    }
  ]
}